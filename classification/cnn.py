# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A-6PAtNLDOsfbE5Raq0K7ZcogbCzJmU9
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
import os

import librosa
import librosa.display
import torch
import torchaudio
import IPython.display as ipd

filename = "/content/drive/MyDrive/Test/audio/fold1/101415-3-0-2.wav"
audio, sr = librosa.load(filename) # 알아서 모노로 바꿔준다, 자동 resampling, 정규화

print(audio) # 소리 세기
print(len(audio))
print('Sampling rate (Hz): %d' %sr)
print('Audio length (seconds): %.2f' % (len(audio) / sr))

ipd.Audio(audio, rate=sr)

plt.figure(figsize =(12,4))
librosa.display.waveplot(y=audio,sr=sr)
plt.show()

metadata = pd.read_csv("/content/drive/MyDrive/Test/UrbanSound8K.csv")
metadata.head()
metadata.sample(n=5)
# print(len(metadata))

"""# MFCC"""

max_pad_len = 1287

def extract_mfcc(file_name):
    try:
        audio, sr = librosa.load(file_name, res_type='kaiser_fast') 
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
        # 패딩
        pad_width = max_pad_len - mfccs.shape[1]
        mfccs = np.pad(mfccs, pad_width = ((0, 0), (0, pad_width)), mode = 'constant')
        #print(mfccs.shape)
    except Exception as e:
        print("Error encountered while parsing file: ", file_name)
        return None 
     
    return mfccs

sample=[]
sample.append(extract_mfcc(filename))

np.shape(sample)

datasetpath = "/content/drive/MyDrive/Test/audio"
audio_mfcc = []

for index, row in metadata.iterrows():
  file_name = os.path.join(os.path.abspath(datasetpath), 'fold'+str(row["fold"])+'/',str(row["slice_file_name"]))
  class_label = row["classID"]
  data = extract_mfcc(file_name)
  audio_mfcc.append([data, class_label])

mfccresult = pd.DataFrame(audio_mfcc, columns=['mfccs','classID'])
print("Finished feature extraction from ", len(mfccresult), " files") 
mfccresult.head()

"""# 데이터셋 생성"""

from keras.utils import to_categorical

X = np.array(mfccresult.mfccs.tolist())
y = np.array(mfccresult.classID.tolist())

print(X.shape)

X.shape

# 원핫 인코딩
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
yy = to_categorical(le.fit_transform(y))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, yy, test_size = 0.2, random_state = 42)

print(X_train.shape)
print(X_test.shape)

import tensorflow as tf

num_rows = 40
num_columns = 1287
num_channels = 1
n_classes = 3

X_train = tf.reshape(X_train, [-1, num_rows, num_columns, num_channels])
X_test = tf.reshape(X_test, [-1, num_rows, num_columns, num_channels])

print(X_train.shape)
print(X_test.shape)

"""# CNN"""

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.utils import np_utils
from sklearn import metrics 

num_labels = yy.shape[1]
filter_size = 2

# CNN 
model = Sequential()

model.add(Convolution2D(filters = 16, kernel_size = 2, input_shape = (num_rows, num_columns, num_channels), activation = 'relu'))
model.add(MaxPooling2D(pool_size = 2))
model.add(Dropout(0.2))

model.add(Convolution2D(filters = 32, kernel_size = 2, activation = 'relu'))
model.add(MaxPooling2D(pool_size = 2))
model.add(Dropout(0.2))

model.add(Convolution2D(filters = 64, kernel_size = 2, activation = 'relu'))
model.add(MaxPooling2D(pool_size = 2))
model.add(Dropout(0.2))

model.add(Convolution2D(filters = 128, kernel_size = 2, activation = 'relu'))
model.add(MaxPooling2D(pool_size = 2))
model.add(Dropout(0.2))

model.add(GlobalAveragePooling2D())
model.add(Dense(num_labels, activation = 'softmax'))

# 다중 분류 문제
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')
model.summary()

score = model.evaluate(X_test, y_test, verbose=0)
accuracy = 100*score[1]

print("Pre-training accuracy: %.4f%%" % accuracy)

model.save('/content/drive/MyDrive/Test/save_models/weights.best.cnn.hdf5')

from keras.callbacks import ModelCheckpoint 
from datetime import datetime 

num_epochs = 300
num_batch_size = 256

# checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/Test/save_models/weights.best.cnn.hdf5', verbose=1, save_best_only=True)
start = datetime.now() 

# model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)
history = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs)

duration = datetime.now() - start
print("Training completed in time: ", duration)

"""# Evaluating"""

import matplotlib.pyplot as plt

def vis(history,name) :
    plt.title(f"{name.upper()}")
    plt.xlabel('epochs')
    plt.ylabel(f"{name.lower()}")
    value = history.history.get(name)
    val_value = history.history.get(f"val_{name}",None)
    epochs = range(1, len(value)+1)
    plt.plot(epochs, value, 'b-', label=f'training {name}')
    if val_value is not None :
        plt.plot(epochs, val_value, 'r:', label=f'validation {name}')
    plt.legend(loc='upper center', bbox_to_anchor=(0.05, 1.2) , fontsize=10 , ncol=1)


def plot_history(history) :
    key_value = list(set([i.split("val_")[-1] for i in list(history.history.keys())]))
    plt.figure(figsize=(12, 4))
    for idx , key in enumerate(key_value) :
        plt.subplot(1, len(key_value), idx+1)
        vis(history, key)
    plt.tight_layout()
    plt.show()
    
plot_history(history)

score = model.evaluate(X_train, y_train, verbose=0)
print("Training Accuracy: ", score[1])

score = model.evaluate(X_test, y_test, verbose=0)
print("Testing Accuracy: ", score[1])

